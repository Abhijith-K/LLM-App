# Open Source Powered LLM - Ollama
Experimenting the Ollama model locally

Prerequisite
1. Install Ollama (mac os)
2. Configured model - gemma3:4b (https://ollama.com/library/gemma3)

## 1. App Fastapi
This streamlit application is integrated with fastapi and ollama model(local).

## 2. App
This is a simple streamlit App

## 3. YT Summarizer
This application will summarize the transcript from the give youtube video url.
